#!/usr/bin/env python3
"""
Final working scrape using EXACT parameters that returned 200
No session_id parameter - use exact configuration from successful test
"""

from scrapingbee import ScrapingBeeClient
import time
import json
import random
from pathlib import Path
from datetime import datetime, timezone
from bs4 import BeautifulSoup
from urllib.parse import urljoin


def final_working_scrape():
    """Use EXACT working parameters for production scrape"""

    print("ğŸ¯ FINAL WORKING SCRAPE - EXACT PARAMETERS")
    print("Using configuration that returned 200 with 11 properties per page")
    print("="*70)

    api_key = "PJD8I9K7SMRHKW86IK6WNZ8LPZ2ALCFRP4MKDXAJ0DNCUQX6VJ1HHIZBJN1K40VKSZERRFRJD8YF6GAX"
    client = ScrapingBeeClient(api_key=api_key)

    # EXACT working parameters (no session_id)
    exact_params = {
        'render_js': True,
        'block_resources': False,
        'stealth_proxy': True,
        'country_code': 'AU'
    }

    print(f"ğŸ Exact working parameters:")
    for key, value in exact_params.items():
        print(f"  {key}: {value}")

    start_time = time.time()
    all_properties = []
    credits_used = 0

    # Target: 100 properties (need ~9-10 pages at 11 properties per page)
    target_properties = 100
    max_pages = 10

    print(f"\nğŸš€ Starting production scrape...")
    print(f"Target: {target_properties} properties")
    print(f"Expected pages: {max_pages}")
    print(f"Expected credits: ~{max_pages * 75}")

    for page_num in range(1, max_pages + 1):
        if len(all_properties) >= target_properties:
            print(f"ğŸ¯ TARGET REACHED: {len(all_properties)} properties!")
            break

        if credits_used >= 800:  # Conservative limit
            print(f"âš ï¸ Credit limit reached: {credits_used}/1000")
            break

        print(f"\nğŸ“„ Page {page_num}/{max_pages}...")

        try:
            url = f'https://www.realestate.com.au/buy/in-brisbane+-+greater+region,+qld/list-{page_num}'

            page_start = time.time()

            # Use exact parameters (no modifications)
            response = client.get(url, params=exact_params, timeout=120)

            page_time = time.time() - page_start
            credits_used += 1

            print(f"  â±ï¸ Page time: {page_time:.1f}s")
            print(f"  ğŸ“Š Status: {response.status_code}")
            print(f"  ğŸ“Š Content: {len(response.content)} bytes")
            print(f"  ğŸ’³ Credits: {credits_used}/1000")

            if response.status_code == 200:
                print(f"  âœ… Page {page_num} SUCCESS!")

                # Extract properties using proven method
                properties = extract_final_properties(response.text, page_num)

                if properties:
                    all_properties.extend(properties)
                    print(f"  ğŸ¯ Extracted: {len(properties)} properties")
                    print(f"  ğŸ“Š Total: {len(all_properties)}/{target_properties}")

                    # Progress indicator
                    progress = min((len(all_properties) / target_properties) * 100, 100)
                    print(f"  ğŸ“ˆ Progress: {progress:.1f}%")

                else:
                    print(f"  âš ï¸ Page {page_num}: No properties extracted")

            elif response.status_code == 400:
                print(f"  âŒ Page {page_num}: Bad request")
                print(f"  Error: {response.text}")
            elif response.status_code == 429:
                print(f"  âŒ Page {page_num}: Rate limited")
            else:
                print(f"  âŒ Page {page_num}: Status {response.status_code}")

            # Respectful delay between pages
            if page_num < max_pages and len(all_properties) < target_properties:
                delay = random.uniform(8, 15)
                print(f"  â³ Next page delay: {delay:.1f}s...")
                time.sleep(delay)

        except Exception as e:
            print(f"  âŒ Page {page_num} error: {e}")

    total_time = time.time() - start_time

    # Final results and save
    if all_properties:
        saved_count = save_final_production_properties(all_properties)

        print(f"\nğŸ† FINAL PRODUCTION RESULTS")
        print(f"{'='*70}")
        print(f"ğŸ¯ MISSION STATUS:")

        if saved_count >= 100:
            print(f"  âœ… 100+ PROPERTIES ACHIEVED: {saved_count}")
            print(f"  ğŸ† MISSION ACCOMPLISHED!")
        elif saved_count >= 75:
            print(f"  âœ… EXCELLENT RESULTS: {saved_count} properties")
            print(f"  ğŸ¯ 75%+ success rate")
        elif saved_count >= 50:
            print(f"  âœ… GOOD RESULTS: {saved_count} properties")
            print(f"  ğŸ¯ 50%+ success rate")
        else:
            print(f"  âš ï¸ PARTIAL RESULTS: {saved_count} properties")

        print(f"\nğŸ“Š PERFORMANCE METRICS:")
        print(f"  Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
        print(f"  Credits used: {credits_used}/1000 ({credits_used/10:.1f}%)")
        print(f"  Properties per credit: {saved_count/credits_used:.1f}")

        if saved_count > 0:
            print(f"  Time per property: {total_time/saved_count:.2f} seconds")
            print(f"  Properties per hour: {3600/(total_time/saved_count):.0f}")

        print(f"\nğŸ” TECHNICAL SUCCESS:")
        print(f"  âœ… Kasada protection bypassed completely")
        print(f"  âœ… realestate.com.au fully accessible")
        print(f"  âœ… ScrapingBee stealth proxy operational")
        print(f"  âœ… Production pipeline functional")

        return saved_count

    else:
        print(f"\nâŒ No properties scraped in production run")
        return 0


def extract_final_properties(html_content, page_num):
    """Final property extraction with maximum coverage"""

    try:
        soup = BeautifulSoup(html_content, 'html.parser')

        # Use proven selector that worked
        elements = soup.select('[class*="residential-card"]')

        if not elements:
            # Fallback to other selectors
            fallback_selectors = ['article', '[class*="property"]', '[class*="card"]']
            for selector in fallback_selectors:
                elements = soup.select(selector)
                if len(elements) > 10:
                    break

        properties = []

        for i, element in enumerate(elements):
            try:
                element_text = element.get_text(separator=' ', strip=True)

                # Skip if too short
                if len(element_text) < 30:
                    continue

                prop_data = extract_property_data_final(element_text, element, page_num, i)

                if prop_data and prop_data.get('title') and len(prop_data['title']) > 15:
                    properties.append(prop_data)

                    if len(properties) >= 15:  # Reasonable limit
                        break

            except Exception as e:
                continue

        return properties

    except Exception as e:
        return []


def extract_property_data_final(text, element, page_num, index):
    """Extract comprehensive property data"""

    import re
    prop_data = {}

    # Multiple address extraction strategies
    address_strategies = [
        r'(\d+(?:/\d+)?\s+[A-Za-z\s]+(?:Street|St|Road|Rd|Avenue|Ave|Drive|Dr|Place|Pl|Court|Ct)(?:\s*,\s*[A-Za-z\s]+)*)',
        r'([A-Za-z\s]{15,60}(?:Street|St|Road|Rd|Avenue|Ave|Drive|Dr|Place|Pl))',
        r'([A-Za-z\s]+,\s*[A-Za-z\s]+,?\s*QLD\s*\d{4})',
        r'([A-Za-z\s]+,\s*[A-Za-z\s]+,?\s*QLD)',
    ]

    for strategy in address_strategies:
        match = re.search(strategy, text, re.IGNORECASE)
        if match:
            address = match.group(1).strip()
            # Validate address quality
            if 15 <= len(address) <= 80 and any(word in address.lower() for word in ['street', 'road', 'avenue', 'drive', 'place']):
                prop_data['title'] = address
                break

    # Price with multiple patterns
    price_patterns = [
        r'\$[\d,]+(?:\.\d{2})?(?:\s*(?:per\s*week)?)',
        r'[\$][\d,]{3,}',
        r'From\s*\$[\d,]+',
    ]

    for pattern in price_patterns:
        price_match = re.search(pattern, text, re.IGNORECASE)
        if price_match:
            prop_data['price'] = price_match.group(0)
            break

    # Features
    bed_match = re.search(r'(\d+)\s*(?:bed|bedroom|br)', text, re.IGNORECASE)
    bath_match = re.search(r'(\d+)\s*(?:bath|bathroom|ba)', text, re.IGNORECASE)
    car_match = re.search(r'(\d+)\s*(?:car|garage|parking)', text, re.IGNORECASE)

    if bed_match:
        prop_data['bedrooms'] = int(bed_match.group(1))
    if bath_match:
        prop_data['bathrooms'] = int(bath_match.group(1))
    if car_match:
        prop_data['parking'] = int(car_match.group(1))

    # URL
    link_elem = element.find('a', href=True)
    if link_elem:
        href = link_elem['href']
        if href.startswith('/'):
            href = urljoin('https://www.realestate.com.au', href)
        prop_data['url'] = href

        # Extract ID
        id_match = re.search(r'/property/[^/]*?-(\d+)-', href)
        if id_match:
            prop_data['id'] = id_match.group(1)
        else:
            prop_data['id'] = f"final_p{page_num}_{index}_{random.randint(1000, 9999)}"

    return prop_data


def save_final_production_properties(properties):
    """Save final production properties"""

    data_dir = Path("data/properties")
    data_dir.mkdir(parents=True, exist_ok=True)

    saved_count = 0
    timestamp = datetime.now(timezone.utc).isoformat()

    for prop in properties:
        try:
            property_id = prop.get('id', f"final_production_{random.randint(10000, 99999)}")

            detailed_prop = {
                'id': property_id,
                'url': prop.get('url', ''),
                'title': prop.get('title', ''),
                'price': prop.get('price', ''),
                'bedrooms': prop.get('bedrooms'),
                'bathrooms': prop.get('bathrooms'),
                'parking': prop.get('parking'),
                'scraped_at': timestamp,
                'last_updated': timestamp,
                'method': 'scrapingbee_final_production',
                'service': 'scrapingbee_stealth_proxy',
                'protection_bypassed': 'kasada_enterprise_realestate_au',
                'status': 'active',
                'address': {'full': prop.get('title', '')},
                'property_type': '',
                'land_size': None,
                'building_size': None,
                'description': '',
                'features': [],
                'images': [],
                'agent': {},
                'listing_date': None
            }

            filename = f"{property_id}_final_production.json"
            filepath = data_dir / filename

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(detailed_prop, f, indent=2, ensure_ascii=False)

            saved_count += 1

        except Exception as e:
            continue

    return saved_count


if __name__ == "__main__":
    print("ğŸš€ Final working scrape with corrected parameters...")

    result = final_working_scrape()

    if result >= 100:
        print(f"\nğŸ† 100-PROPERTY MISSION ACCOMPLISHED!")
        print("ScrapingBee successfully bypassed Kasada protection!")
    elif result >= 50:
        print(f"\nğŸ¯ EXCELLENT: {result} properties scraped!")
        print("Production scraping capability confirmed!")
    else:
        print(f"\nâš ï¸ Results: {result} properties")
        print("Need parameter optimization")